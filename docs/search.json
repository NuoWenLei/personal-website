{
  "articles": [
    {
      "path": "about.html",
      "title": "About",
      "author": [],
      "contents": "\nAbout Me\n\n\n\n\nI’m Nuo Wen Lei from Shanghai, studying in the 2026 class of Brown University.\nI have an interest in Computer Science, Data Science, and Behavioral Sciences, and I like to play tennis and squash, watch anime, and eat delicious food!\n\nWays to reach me:\nGithub\nLinkedIn\nEmail: nuowen0612@gmail.com\nInstagram: nuowenlei\nDiscord: NuoWenLei#0308\nSports:\nVarsity Tennis First Singles in CA. Here’s my tennis video\nVarsity Squash Number 1 in CA.\nMy Journey in High School\nBefore 2020, my experience with coding had been in various kinds of sandbox environments. Inputs were cleanly defined and outputs bore no real meaning or weight. It was a low point for my interest in programming because nothing I made seemed to make a difference.\nIn fact, all the projects presented on this website were created after 2020. Because of COVID, our school swiftly adjusted to remote learning, however the quick transition left many teachers under-prepared for the curriculum. There were many long days with little to do at home, so I decided to take up online learning, starting with Harvard’s CS50x. That was the spark to my interest in programming.\nAfter completing CS50x rather quickly, I took the web development progression of the course, CS50w. In that class, I learned about both front-end development with HTML, CSS, Javascript and back-end with Flask, Django, and SQL.\nI was really excited after gaining this new-found knowledge because I was finally able to write programs that went beyond the sandbox. The first project I created was TakeActionMass in June 2020. Not only was it the culmination of what I had learned in the past 3 months, it also addressed a problem, it sought to make a difference. I’m really proud of it.\nDuring the process of making that website, I encountered various problems with both front-end and back-end. Back-end especially, I had to struggle through a lot of online database problems and understanding its limitations. This struggle actually sparked my interest in the manipulation and organization of data.\nIn September of 2020, as my Junior year started, I decided to also enroll in University of Michigan’s Data Science Specialization with Python on Coursera. It was a 5-course program, covering numerous topics including data processing, visualization, and machine learning models. I learned about various tools in Python that helped a lot with the understanding and processing of data, and also worked through different notebooks each about important problems.\nIt was also at this time that I started my investment bot project. Its main purpose is to automatically collect stock market data that I would be able to use in future projects, but I also added a simple investment simulation part just to see how well it can do.\nAs 2021 came along, I kickstarted my second Data Science specialization on Coursera with IBM, learning more about probability, data processing, and machine learning. As a final project, I wrote the analysis report and created the presentation on COVID death trends.\nIt was through these courses that my interest in ML and AI really grew. I began enrolling in extracurricular programs about AI, mainly InspiritAI, and I learned a lot about the concepts and implementation behind AI. By then, it was March, and I created and participated in various projects including: Epilepsy Detection, Mask Detection, Explorantine, WangWang, and Shanghai Property Analysis.\nThese laid the groundwork for my further exploration into the field of Data Science and AI during the summer of 2021. I enrolled in a Data Science course and created the PAC analysis website as well as contributed to the R tutorial package.\nThis is the story of my journey so far. As Senior year and college applications approach, I hope I can still make the time to work on these projects because it has really been a transforming experience.\n\n\n\n",
      "last_modified": "2023-05-12T13:37:15-04:00"
    },
    {
      "path": "certificates.html",
      "title": "Certificates",
      "author": [],
      "contents": "\nIn pursuing my interests in Computer Science, Data Science, and Artificial Intelligence, I completed various online certifications with different universities and organizations.\nData Science Certifications\nIBM - Advanced Data Science\nUniversity of Michigan - Applied Data Science with Python\nDeepLearning.AI - Machine Learning Engineering for Production (MLOps)\nWeb Programming Certifications\nHarvard CS50w - Web Programming with Python and Javascript\nBasic Programming Certifications\nHarvard CS50x\nMicrosoft Technology Associate - Introduction to Programming using Python\n\n\n\n",
      "last_modified": "2023-05-12T13:37:16-04:00"
    },
    {
      "path": "covid_analysis.html",
      "title": "COVID Death Trend Analysis and AI Prediction",
      "author": [],
      "contents": "\nAssociated Links:\nCovid Analysis AI Report + Github Repo\nIBM Data Science Course Presentation\nThis project started off as my final project for an IBM Data Science MOOC course. I used basic AI methods like Long Short Term Memory (LSTM) models to try to predict COVID death trends.\n\n\nMy final presentation is posted on YouTube, and it shows the process I went through to extract and construct the data as well as suggest a basic AI solution.\n\n\n\nFigure 1: https://github.com/NuoWenLei/covid_tracking_by_state#readme\n\n\n\nHowever, after the course, I wanted to see if I could create a way that produced better numbers in predicting trends, so I dove deeper into more innovative data-processing methods and documented my full process and results on this Github page.\n\n\n\n",
      "last_modified": "2023-05-12T13:37:16-04:00"
    },
    {
      "path": "cubby.html",
      "title": "Cubby",
      "author": [],
      "contents": "\nAssociated Links:\nCubby website\nCubby API\nGitHub\nThis project stemmed from my desire to utilize Machine Learning techniques in a web development project. I wanted to cross my two fields of interest within Computer Science into a project and that’s how the idea of Cubby was born!\n\n\n\nFigure 1: Landing page of Cubby\n\n\n\nThe timeframe for this project was very tight due to the limited 3 weeks we had before the end of school but we made it work with an incredible team that had many specialists in various areas.\n\n\n\nFigure 2: Matching page of Cubby\n\n\n\nThe frontend design was made by a teammate who was a senior at Brown that had learned about UI/UX and taken multiple design courses, and therefore she had the experience with 3D rendering and various amazing design choices! While I tried my best to implement her designs, we could not fit in some features like dynamically generated images within the time frame we were given.\n\n\n\nFigure 3: Cubby FastAPI page\n\n\n\nThe backend was made by me using the FastAPI Python package. It utilizes pretrained word embeddings from GloVe to encode questionnaire answers into embeddings, which allows us to use KMeans clustering to group people together by the semantic similarity of their questionnaire answers. However, in order to add an element of randomness (so the matched groups are not always with the same people), I chose to randomly sample a limited number of dimensions from the encoded vectors to cluster on.\n\n\n\nFigure 4: Cubby Community page\n\n\n\nWe also wanted to give people the option to form their own groups, so we also created a communities page where people can create and join communities. An interesting technical detail is that we implemented the search function to match community names with the query based on Cosine Similarity, which is different from usual search algorithms.\n\n\n\nFigure 5: Cubby Group page\n\n\n\nIn conclusion, this project was a joy to work on and my teammates was very collaborative and timely with their tasks. While I am not sure if we will expand this idea any further, this remains an interesting project that I personally believe has a lot of potential. And it is something that I may revisit someday!\n\n\n\n",
      "last_modified": "2023-05-12T13:37:17-04:00"
    },
    {
      "path": "epilepsy_webapp.html",
      "title": "Epilepsy Detection AI Web App",
      "author": [],
      "contents": "\nAssociated Links:\nEpilepsy Detection Web App\nInspirit AI Main Page\nWith friends from an online AI course, Inspirit AI, we trained an AI to detect if a patient is experiencing epilepsy based on their EEG brain wave graph.\n\n\n\nFigure 1: Output of AI shows its confidence in its decision while processing the graph\n\n\n\nWe were able to reach an accuracy of 89%, which is definitely not accurate enough to be used professionally. However, throughout the project we learned a lot about the ethical concerns surrounding AI’s influence on major fields as well as problems with modern-day AI’s explain-ability that discourage people to trust its results.\nIf you are interested, please do click here to try it out for yourself. Note that it does take a while to load because the AI model size is big.\n\n\n\n",
      "last_modified": "2023-05-12T13:37:18-04:00"
    },
    {
      "path": "explorantine.html",
      "title": "BB&N Hackathon Winning Project - Explorantine",
      "author": [],
      "contents": "\nAssociated Links:\nDevpost Project Showcase\nGithub Repo\nOnline Presentation of Hackathon Project\nJoined by friends from Concord Academy and Middlesex, our team of 4 took on a one-day hackathon of creating a proof-of-concept web project.\nMy teammates were:\nShreya Jain\nCaspian Ahlberg\nRachel Hu\nOne of the awards was “Best Small Business Solution”, which was the award we were aiming for. Since small businesses were losing a lot of customers during the pandemic, we decided to make a web app centered around supporting local businesses. It allowed users to search for nearby businesses based on their mood or desired activities.\n\n\n\nFigure 1: Main Page of Explorantine\n\n\n\nThe page returns a list of places that are near your location. However, since this is a proof of concept, there are a lot of rough edges and room for improvement with the results.\n\n\n\nFigure 2: Main Page of Explorantine\n\n\n\nUsing Natural Language Processing techniques with NLTK and spaCy as well as the Google Places API, we were able to match moods based on user input and search for places nearby that fit the user’s desire. It was the combination of a lot moving parts but with 10 hours straight of working on it, we got it working.\n\n\nIn the end, we were able to not only win “Best Small Business Solution” but also “Best Overall”! It was extremely fun to work on this project with friends, and if the opportunity presents itself in the future, I’d gladly refine and hopefully publish this project to the world!\n\n\n\n",
      "last_modified": "2023-05-12T13:37:19-04:00"
    },
    {
      "path": "index.html",
      "title": "Personal Projects",
      "author": [],
      "contents": "\nAbout\n\n\n\n\nHello, I’m Nuo Wen Lei from Shanghai, current studying at Brown University, class of 2026. My personal interests mainly lie in the realm of understanding humanities and the world around me through the lens of data and logic.\n\nI’m also a tennis and squash player (clip of me playing tennis) as well as a Japanese-cuisine appreciator with ramen and chicken wings being my favorite.\n\nThis website is a gallery of the projects I’ve done. It shows how my interests developed over time from computer science to web design to data science and AI.\n\nI hope you find something interesting or just enjoy your time on this website!\n\nTable of Contents\nCOVID Death Trend Analysis and AI Prediction\nCubby - Friend Data Matcher\nEpilepsy Detection AI Web App\nHackathon Winning Project - Explorantine\nInvestment Bot\nMask Detection AI Web App\nPAC Donation Analysis Website\nR Tutorial Package Contribution\nSenior Project - Neural Network Interpreter\nShanghai Property Analysis Website\nWeChat Miniprogram - WangWang\nWordle and Quordle Solver\nQuick Overview\nGo to Projects tab for more details on each individual project\n\n\n\n\nCOVID Death Trend Analysis and AI Prediction  After an initial analysis of the COVID death trends and trial of basic AI methods, I dive deeper into finding other AI techniques to predict the death trends.\n\n\n\n\n\n\n\nEpilepsy Detection AI Web App  I hosted an AI I created with friends that determines if someone is experiencing epilepsy based on EEG brain waves.\n\n\n\n\n\n\n\nHackathon Winning Project - Explorantine  Our Hackathon team created a web app to help local businesses bring in customers.\n\n\n\n\n\n\n\nInvestment Bot  I created a pipeline that analyzes web data and invests on an Investopedia simulator.\n\n\n\n\n\n\n\nMask Detection AI Web App  I created and hosted an AI on the internet that detects if people are wearing masks in an image.\n\n\n\n\n\n\n\nPAC Donation Analysis Website  I analyzed and visualized the relationship between foreign-connected company donations with US Congress representation.\n\n\n\n\n\n\n\nR Tutorial Package Contribution  Over the summer of 2021, I work with a professor to help improve an R tutorial package.\n\n\n\n\n\n\n\nSenior Project - Model Interpreter  For my final spring semester in high school, I aimed to create a neural network interpreter as my senior project.\n\n\n\n\n\n\n\nShanghai Property Analysis Website  I created a website that visualizes the relationship between properties and position in Shanghai.\n\n\n\n\n\n\n\nWeChat Miniprogram - WangWang  With friends, we are creating a WeChat miniprogram accessible to all WeChat users that can match tennis players with others who play in the same area.\n\n\n\n\n\n\n\nWordle and Quordle AND OCTORDLE Solver  As a fun spring break project, I created an algorithm that can live solve Wordle, Quordle, and Octordle with probability.\n\n\n\n\n\n\n\nCubby  As part of my software engineering class (CS0320), my team created a data matching website that matches people into friend groups based on similarities in questionnaire answers.\n\n\n\n\n",
      "last_modified": "2023-05-12T13:37:20-04:00"
    },
    {
      "path": "investment_bot.html",
      "title": "Investment Bot",
      "author": [],
      "contents": "\nThis project started off as a personal challenge. During 2020, I took a lot of online courses through sites like EdX and Coursera ranging from web development to data science.\nI’ve made websites at this point, but I’ve never really applied my study in data science. This was essentially my first data science project.\n\n\n\nFigure 1: Yahoo Finance page for the stock AXS\n\n\n\nUsing python and its various libraries, I created a simple pipeline to extract and analyze online data and then invest based on its results. Then I attached this pipeline to a web server that will run this pipeline every week.\n\n\n\nFigure 2: Visualization of investment pipeline\n\n\n\nData Webscrape - The pipeline starts by taking various numerical and textual data from multiple online sources.\nData Cleaning and Processing - The data is then cleaned for processing use since these online sources meant for the data to be presented to people and not machines. In this section, features like market cap values are also created based on their non-numeric displays on websites.\nInvestment Decision with ML - Using previously collected data, many different ML models are trained, each with a different model-scaler pairing. The pair with the highest evaluation score is selected to transform the new data. Based on the confidence level of the predictions, the pipeline invests in the top 5 stock symbols on an Investopedia simulator\n\n\n\n",
      "last_modified": "2023-05-12T13:37:20-04:00"
    },
    {
      "path": "mask_webapp.html",
      "title": "Mask Detection AI Web App",
      "author": [],
      "contents": "\nAssociated Links:\nMask Detection Web App\nImage-processing AI Concept Presentation\nBy combining a face detection model and my personal mask detection model, I created an AI that can individually detect for each person in a photo if they are wearing a mask.\n\n\n\nFigure 1: Preview of uploaded image before processing\n\n\n\nI then hosted this AI on a website for people to try out for themselves. I also presented this project as a demo for a presentation about teaching AI concepts to CA’s robotics club members.\nFeel free to try this AI out for yourself here. Note that it does take a while to load because the AI model size is big.\n\n\n\nFigure 2: Result after mask detection AI processed image\n\n\n\n\n\n\n",
      "last_modified": "2023-05-12T13:37:21-04:00"
    },
    {
      "path": "pac_analysis.html",
      "title": "PAC Donation Analysis",
      "author": [],
      "contents": "\nAssociated Links:\nPAC Influence Project Website\nOpenSecrets\nFor the final project of a data science course about R, I picked my own topic of research to be about foreign-connected Political Action Committee donations for US political parties.\nPlease feel free to visit the project page and see for yourself.\n\n\n\nFigure 1: Data source for the project\n\n\n\nIn the project, I scrape data from OpenSecrets and process it to produce visualizations comparing donation amounts and Congress representations between 2000 and 2020.\n\n\n\n",
      "last_modified": "2023-05-12T13:37:22-04:00"
    },
    {
      "path": "r_package.html",
      "title": "R Data Science Tutorial Package Contribution",
      "author": [],
      "contents": "\nAssociated Links:\nPreceptor’s Primer for Bayesian Data Science\nData Science Course hosted by professor David Kane\nClass Management Documentation\nAssignment Management Documentation\nAll my code contributions\nAt the end of the 2021 school year, I joined a free data science course hosted by professor David Kane. During which he reached out to me to ask if I’d like to work with him over the summer to help improve his R tutorial package, Preceptor’s Primer for Bayesian Data Science.\n\n\n\nFigure 1: Chapter 1 of the Primer\n\n\n\nI gladly joined him with a few other volunteers. I started off by just catching and editing small mistakes in the tutorials reported by users, then I got the opportunity to write the tutorial, Data Webscraping, myself. And afterwards, I was allowed to make some larger changes to tutorials in general.\n\n\n\nFigure 2: Side navigation bar of the Data Webscraping tutorial\n\n\n\nFirst, I was assigned to add some quality-of-life features for better user experience. Then I helped make the process of creating tutorials more streamlined and simple by adding pre-defined templates and keyboard shortcuts to take the tedious work of writing each exercise individually off the tutorial-makers.\n\n\n\nFigure 3: My Github contributions to the package\n\n\n\nAfterwards, I worked on fixing a lot of the package’s technical errors while adding more addins that check and format tutorials. Also, a recent feature I am working on is creating a report of submitted answers for users to look at after completing each tutorial.\n\n\n\nFigure 4: Sample Report\n\n\n\nAs the summer came to an end, I shifted towards working more on general class and assignment management programs. With Discord templates and bots, I built a class system that allowed classes to be organized and conducted over Discord in a similar fashion as over Zoom.\n\n\n\nFigure 5: Class Discord Server Template\n\n\n\nAnd the final assignment management features I added were what I thought to be my greatest contributions to the package. Using R packages that allowed remote and automated access to Gmail and Google Drive, I created an assignment collection, organization, and summarization system. With a single function, my code was able to collect the submissions from someone’s Gmail, downloaded them, and aggregate user information such as name, email, and time spent. After processing the submissions, the aggregated data and submissions are uploaded to their Google Drive, allowing for easy access to the tallied information.\n\n\n\nFigure 6: Class Discord Server Template\n\n\n\nIt was truly an amazing experience to work so closely with professor David Kane over the summer. If there is ever another chance for me to do meaningful work with him, I would gladly do so!\n\n\n\n",
      "last_modified": "2023-05-12T13:37:23-04:00"
    },
    {
      "path": "senior_project.html",
      "title": "Senior Project - Neural Network Interpreter",
      "author": [],
      "contents": "\nAssociated Links:\nGitHub Repo\nSchool Presentation\nOPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING\nATTENTION IS ALL YOU NEED\nThis is the senior project that I dedicated most of my spring semester to. The focus is on creating a proof-of-concept AI model that can interpret another AI model. Using word embeddings to understand features and weights and biases between layers to understand relationships between data points and neurons, I wanted to make an AI model that can “translate” the AI neuron weights to english, cracking open the neural network blackbox.\n\n\n\nFigure 1: Interpreter Pipeline Structure\n\n\n\nThe way I landed on this particular idea was kind of through a mishmash of what I wanted to try and what problem I wanted to solve (mostly just what I wanted to try). I have always wanted to try making models that processed and produced the weirdest inputs and outputs. It just seems like when building models that processed images or sequences, I just slap on some CNNs or LSTMs and then call it a day, rarely contributing anything of my own. Therefore, building this interpreter model is in a way like a challenge for myself. However, that is not to say there is no application for a model like this.\n\n\n\nFigure 2: Difference in model interpretability between Logistic Regression (ML) and Neural Network (DL)\n\n\n\nThe image above compares the difference in model complexity and therefore interpretability between a logistic regression model and a neural network model. You can see that for logistic regression or any kind of direct regression model for that matter, each input feature is assign a single weightage, representing how that feature contributes to the output.\nHowever look to the right and you’ll see that it’s not as straightforward for the neural network. If every line (edge) represents a weightage, you’ll see that the neural network has quite a few more weightages than the logistic regression model. This is largely due to the hidden layers between the inputs and output. These hidden layers contain their own neurons whose meanings are defined and refined by the model. Therefore, encoded as numbers that only the model that created it can understand, neural networks become like blackboxes, incomprehensible to humans.\nThis interpretability issue actually does impact company decisions to utilize and “trust” these machines because often times, especially in recent years, the question of equity has been raised to many big data models. Companies need to make sure that their models do not treat demographics unequitably, and so clarity within the model is very important. The neural network’s lack in that area thus often push big data companies away from using deep learning solutions and resorting to more basic machine learning solutions that produce decent results with clarity of feature importance.\nThis is the problem I wanted to solve with this model, some kind of mediator that can tell humans what the models are thinking.\n\n\n\nFigure 3: Word Embedding Visualization\n\n\n\nSo to make my model understand words, I encoded all the feature names into word embeddings, which are vector points that represent word relationships within a 300-dimensional vector space. To let my model understand word embeddings, I utilized the Multi Head Attention layer proposed in the paper, ATTENTION IS ALL YOU NEED and performed self attention with the layer. This allows the model to mix and understand how every dimension of the word embedding vectors relate to each other, thereby letting the model gain an even more comprehensible understanding of the features.\n\n\n\nFigure 4: Multi Head Attention layer structure\n\n\n\nBy combining the understanding of words and linear weights, I created 2 model types based on the mutability of input order. When thinking about how to input the data, I realized that the model shouldn’t need to have an immutable input order, as in an arbitrary feature “x” shouldn’t always have to be in the same position of the batch. Therefore I first created a model structure that didn’t need a determined input order by treating one data sample with all the different features and weights as a batch with each sample being the corresponding feature and weight.\n\n\n\nFigure 5: Interpreter with dynamic input order\n\n\n\nThen, to see if an interpreter that assumes static input order yields any benefits, I created a static version too. This static version also takes into account a covariance matrix between features and feature weights since it can assume static input order. The point of the matrix is that it should allow the interpreter to get a ballpark of where the output feature should be in word embedding vector space.\n\n\n\nFigure 6: Interpreter with static input order\n\n\n\nIn terms of results, the static interpreter succeeded in training, continuing to improve even after some 250 steps while the dynamic interpreter seems to have plateaued just after some 50 steps of training.\n\n\n\nFigure 7: Training results of the two interpreters\n\n\n\nHowever, after actually putting the interpreters into a testing environment, the results qualitatively changed drastically.\n\n\n\nFigure 8: Prediction results of the two interpreters\n\n\n\nBoth were given a base model that uses many attributes of a wine to predict the residual sugar in the wine. While the static interpreter repeatedly predicted words that had little to no relation with “residual sugar”, the dynamic interpreter has predicted words that are similar to, if not exactly, “residual sugar”. I think the difference in training and prediction environment results is a testament to how overfitting can be such a misleading problem.\nAnyways, to conclude, this was a super fun project, guiding myself through everything. I feel like that proof-of-concept stage has been reached through this project. And while there are many ways to further this project, I don’t think I am skilled enough to pursue many of those options. So I’ll put the further exploration of this project on hold and see if I can tackle it any better when I return to it.\nIf you want to see my presentation for this project, you can click here.\nAnd here is the GitHub repo with code for the project.\n\n\n\n",
      "last_modified": "2023-05-12T13:37:24-04:00"
    },
    {
      "path": "shanghai_property.html",
      "title": "Shanghai Property and Position Analysis",
      "author": [],
      "contents": "\nAssociated Links:\nShanghai Property Analysis Project Page\nAs part of a school course about urban planning, I decided to analyze the surrounding area of where I lived. It first started off as a small area analysis, but as my interest grew into Shanghai’s urban planning and as my direction of research became clearer, I scraped data from online property-selling sites to produce a map that visualizes different attributes of properties and their positions in Shanghai.\n\n\n\nFigure 1: Distribution of district property colored by year of construction (red: old, green: new)\n\n\n\nUsers can choose what attribute of the property to visualize and gain an insightful overview of the city distribution.\n\n\n\nFigure 2: Distribution of district property colored by average price per square meter (red: low, green: high)\n\n\n\nWith this map, I also considered stakeholders and their interconnectivity in this area.\n\n\n\nFigure 3: Graph of profit relationships between stakeholders in the Shanghai community\n\n\n\nOverall, I’d say this project in the scope of my school course was a success. Through this experience, I’ve learned that the world of urban planning is vast. This project has only glimpsed at the peak of the iceberg and if opportunities arise in the future for me to dive deeper into the hidden effects and influences of urban planning, I’d gladly pursue it!\n\n\n\n",
      "last_modified": "2023-05-12T13:37:25-04:00"
    },
    {
      "path": "wangwang.html",
      "title": "WeChat Miniprogram - WangWang",
      "author": [],
      "contents": "\nA good friend and I started this project because we were both tennis players who had a hard time finding new players to play with near us. Therefore, we decided to make a WeChat miniprogram that does exactly that. We decided to create it as a WeChat miniprogram to reach a large audience because WeChat is the main way of communication in China.\n\n\n\nFigure 1: Main Page of WangWang\n\n\n\nI handled the javascript backend programming to connect with the cloud database and my friend did all the styling and position for the frontend. As we continued the project, another friend joined us and designed the layouts for all the pages. We are still currently working on a development version, so unfortunately it is not usable yet, but we have created several features already.\n\n\n\nFigure 2: Court Search Page of WangWang\n\n\n\nThe main feature is searching for courts near you, which gives you information about the court location and rating as well as how many people play there.\n\n\n\nFigure 3: Profile Page of WangWang\n\n\n\nAnother feature we added is a personal profile page, which allows you to fill in your basic information for matching optimal tennis partners.\n\n\n\nFigure 4: Calendar Page of WangWang\n\n\n\nA feature we are still working on is a calendar function, which will allow users to better schedule meetings as well as further filter players based on parameters like available time.\nThough this project is still a work-in-progress, I have high hopes for this miniprogram and I wish that in the near future I can also personally use it!\n\n\n\n",
      "last_modified": "2023-05-12T13:37:26-04:00"
    },
    {
      "path": "wordle.html",
      "title": "Wordle and Quordle AND OCTORDLE Solver",
      "author": [],
      "contents": "\nAssociated Links:\nGitHub Repo\nWordle\nWordle Archive\nQuordle\nThis is a fun project I played around with over my senior year spring break. As the New York Times Wordle game took my school by storm, its simple format made me curious about the possibility of creating an algorithm to solve it. To start the project, I knew that I had to first create some interface for a machine to play Wordle, so, as pastime on a flight, I made a text version of Wordle. There is a small difference however. While the actual Wordle often displays words that are well-known to people, I made my Wordle simply randomly choose a word out of all possible 5-letter words.\n\n\n\nFigure 1: Text Version of Wordle\n\n\n\nIn the text version, capitalize means the letter is in the right place, “*” means the letter is present but in the wrong location, and lower case means the letter is not in the word. I also added a “Possible Letters” bar for ease of playing.\nAfter creating the Wordle game, I began to think about how I would approach creating a Wordle solver. Part of me wanted to create a Reinforcement Learning agent to play the game, but another part of me felt that I’ve been doing too much AI-related projects recently. I settled on creating a non-AI algorithm that would probabilistically choose the optimal words.\n\n\n\nFigure 2: Exploration of letter-location probability at the first position of a word\n\n\n\nI first made an algorithm that assigned information “rewards” to getting a letter correct (3 points), present (2 points), or wrong (1 point), thereby generating an information reward value for each word in the vocabulary. It then simply chose the word with the highest information reward. This algorithm created a machine that prioritized letters with a high probability of being in that position above all else. And while it did work to an extent, it was more of a Wordle “attempter” than a Wordle “solver”.\nNow if I really wanted to spin this into a story about personal insights and biases, this would be the moral of the story. I assumed that a letter in the correct place would always provide more information than other results, and in doing so I created an algorithm whose goal deviated from the overarching goal.\nTherefore, to improve the machine, firstly I changed the information reward as dependent on what percentage of the vocabulary the action would eliminate. For example, if getting the word correct eliminates half the vocab, its reward for being correct was 1/2. This is somewhat of a greedy solution because by evaluating the effect of each letter individually, I ignore the percentage of vocabulary that would be eliminated by the outcomes of 2 or more letters in the word. This overlap-elimination effect, however, is minimal, so I chose to ignore the flaw.\nSecondly, I added a bunch of conditionals to make sure that no information was wasted. I found that covering all the scenarios where a letter that’s present but in the wrong position was quite challenging. But one-by-one I found and covered all those edge cases. Finally, I set the machine to use the first 5 guesses to eliminate as much of the vocabulary as possible and then use the most possible word only for the final guess. This allowed the algorithm to truly “solve” Wordle.\nI then created a web interface for the machine to actually play the official Wordle. Using the Python module of Selenium, the Wordle solver (named Trish) could solve the Wordle live in front of your eyes!\n\n\n\nFigure 3: Trish solving Wordle for March 20, 2022\n\n\n\nIt can also solve any Wordle archived here.\n\n\n\nFigure 4: Trish solving Wordle for March 20, 2022\n\n\n\nAs a final challenge, I created a separate algorithm that solved Quordle instead of Wordle (named Trisha) by simply adding up the information reward value that could be gained from every board and left the last 4 guesses for optimal words. And, well, as of now (March 20, 2022), it hasn’t failed yet!\n\n\n\nFigure 5: Trisha solving Quordle for March 20, 2022\n\n\n\nDid I say final challenge? I lied. A good friend told me that an Octordle exists, so I had to try it out. The previous algorithm actually FAILED on its first attempt to solve the daily, and I realized it was because the model didn’t prioritize game boards with more vocabulary to filter. Therefore, I added a scaling component that multiplies with the information reward based on the number of vocabulary for each game board. This changes the reward gotten from each game board, allowing the solver (now named Patricia) to have a sense of priority or urgency.\n\n\n\nFigure 6: Patricia solving Octordle for March 20, 2022\n\n\n\nThis just started off as a fun project for over spring break. However, it not only helped me brush up on actual hand-coding, but it also taught me some good lessons about probabilities and my own assumptions!\n\n\n\n",
      "last_modified": "2023-05-12T13:37:27-04:00"
    }
  ],
  "collections": []
}
