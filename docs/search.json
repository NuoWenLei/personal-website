{
  "articles": [
    {
      "path": "about.html",
      "title": "About",
      "author": [],
      "contents": "\nAbout Me\n\n\n\n\nI’m Nuo Wen Lei from Shanghai, studying in the 2026 class of Brown\nUniversity.\nI have an interest in Computer Science, Data Science, and Behavioral\nSciences, and I like to play tennis and squash, watch anime, and eat\ndelicious food!\n\nWays to reach me:\nGithub\nLinkedIn\nEmail: nuowen0612@gmail.com\nInstagram: nuowenlei\nDiscord: NuoWenLei#0308\nSports:\nVarsity Tennis First Singles in CA. Here’s my tennis video\nVarsity Squash Number 1 in CA.\nMy Journey in High School\nBefore 2020, my experience with coding had been in various kinds of\nsandbox environments. Inputs were cleanly defined and outputs bore no\nreal meaning or weight. It was a low point for my interest in\nprogramming because nothing I made seemed to make a difference.\nIn fact, all the projects presented on this website were created\nafter 2020. Because of COVID, our school swiftly adjusted to remote\nlearning, however the quick transition left many teachers under-prepared\nfor the curriculum. There were many long days with little to do at home,\nso I decided to take up online learning, starting with Harvard’s CS50x.\nThat was the spark to my interest in programming.\nAfter completing CS50x rather quickly, I took the web development\nprogression of the course, CS50w. In that class, I learned about both\nfront-end development with HTML, CSS, Javascript and back-end with\nFlask, Django, and SQL.\nI was really excited after gaining this new-found knowledge because I\nwas finally able to write programs that went beyond the sandbox. The\nfirst project I created was TakeActionMass in June 2020. Not only was it\nthe culmination of what I had learned in the past 3 months, it also\naddressed a problem, it sought to make a difference. I’m really proud of\nit.\nDuring the process of making that website, I encountered various\nproblems with both front-end and back-end. Back-end especially, I had to\nstruggle through a lot of online database problems and understanding its\nlimitations. This struggle actually sparked my interest in the\nmanipulation and organization of data.\nIn September of 2020, as my Junior year started, I decided to also\nenroll in University of Michigan’s Data Science Specialization with\nPython on Coursera. It was a 5-course program, covering numerous topics\nincluding data processing, visualization, and machine learning models. I\nlearned about various tools in Python that helped a lot with the\nunderstanding and processing of data, and also worked through different\nnotebooks each about important problems.\nIt was also at this time that I started my investment bot project. Its main purpose\nis to automatically collect stock market data that I would be able to\nuse in future projects, but I also added a simple investment simulation\npart just to see how well it can do.\nAs 2021 came along, I kickstarted my second Data Science\nspecialization on Coursera with IBM, learning more about probability,\ndata processing, and machine learning. As a final project, I wrote the\nanalysis report and created the presentation on COVID death trends.\nIt was through these courses that my interest in ML and AI really\ngrew. I began enrolling in extracurricular programs about AI, mainly\nInspiritAI, and I learned a lot about the concepts and implementation\nbehind AI. By then, it was March, and I created and participated in\nvarious projects including: Epilepsy\nDetection, Mask Detection, Explorantine, WangWang, and Shanghai Property Analysis.\nThese laid the groundwork for my further exploration into the field\nof Data Science and AI during the summer of 2021. I enrolled in a Data\nScience course and created the PAC analysis\nwebsite as well as contributed to the R\ntutorial package.\nThis is the story of my journey so far. As Senior year and college\napplications approach, I hope I can still make the time to work on these\nprojects because it has really been a transforming experience.\n\n\n\n",
      "last_modified": "2023-01-17T23:18:00+08:00"
    },
    {
      "path": "certificates.html",
      "title": "Certificates",
      "author": [],
      "contents": "\nIn pursuing my interests in Computer Science, Data Science, and\nArtificial Intelligence, I completed various online certifications with\ndifferent universities and organizations.\nCoursera Certifications\nIBM -\nAdvanced Data Science\nUniversity\nof Michigan - Applied Data Science with Python\nDeepLearning.AI\n- Machine Learning Engineering for Production (MLOps)\nEdX Certifications\nHarvard\nCS50X\nHarvard\nCS50W - Web Programming\nOther Certifications\nMicrosoft\nTechnology Associate\n\n\n\n",
      "last_modified": "2023-01-17T23:18:01+08:00"
    },
    {
      "path": "covid_analysis.html",
      "title": "COVID Death Trend Analysis and AI Prediction",
      "author": [],
      "contents": "\nAssociated Links:\nCovid\nAnalysis AI Report + Github Repo\nIBM Data\nScience Course Presentation\nThis project started off as my final project for an IBM Data Science\nMOOC course. I used basic AI methods like Long Short Term Memory (LSTM)\nmodels to try to predict COVID death trends.\n\n\nMy final\npresentation is posted on YouTube, and it shows the process\nI went through to extract and construct the data as well as suggest a\nbasic AI solution.\n\n\n\nFigure 1: https://github.com/NuoWenLei/covid_tracking_by_state#readme\n\n\n\nHowever, after the course, I wanted to see if I could create a way\nthat produced better numbers in predicting trends, so I dove deeper into\nmore innovative data-processing methods and documented my full process\nand results on this\nGithub page.\n\n\n\n",
      "last_modified": "2023-01-17T23:18:02+08:00"
    },
    {
      "path": "epilepsy_webapp.html",
      "title": "Epilepsy Detection AI Web App",
      "author": [],
      "contents": "\nAssociated Links:\nEpilepsy\nDetection Web App\nInspirit AI Main Page\nWith friends from an online AI course, Inspirit AI, we trained an AI to\ndetect if a patient is experiencing epilepsy based on their EEG brain\nwave graph.\n\n\n\nFigure 1: Output of AI shows its confidence in its decision\nwhile processing the graph\n\n\n\nWe were able to reach an accuracy of 89%, which is\ndefinitely not accurate enough to be used professionally. However,\nthroughout the project we learned a lot about the ethical concerns\nsurrounding AI’s influence on major fields as well as problems with\nmodern-day AI’s explain-ability that discourage people to trust its\nresults.\nIf you are interested, please do click\nhere to try it out for yourself. Note that it does take a\nwhile to load because the AI model size is big.\n\n\n\n",
      "last_modified": "2023-01-17T23:18:02+08:00"
    },
    {
      "path": "explorantine.html",
      "title": "BB&N Hackathon Winning Project - Explorantine",
      "author": [],
      "contents": "\nAssociated Links:\nDevpost\nProject Showcase\nGithub\nRepo\nOnline Presentation of\nHackathon Project\nJoined by friends from Concord Academy and Middlesex, our team of 4\ntook on a one-day hackathon of creating a proof-of-concept web\nproject.\nMy teammates were: Shreya\nJain Caspian Ahlberg Rachel Hu\nOne of the awards was “Best Small Business Solution”, which was the\naward we were aiming for. Since small businesses were losing a lot of\ncustomers during the pandemic, we decided to make a web app centered\naround supporting local businesses. It allowed users to search for\nnearby businesses based on their mood or desired activities.\n\n\n\nFigure 1: Main Page of Explorantine\n\n\n\nThe page returns a list of places that are near your location.\nHowever, since this is a proof of concept, there are a lot of rough\nedges and room for improvement with the results.\n\n\n\nFigure 2: Main Page of Explorantine\n\n\n\nUsing Natural Language Processing techniques with NLTK and spaCy as\nwell as the Google Places API, we were able to match moods based on user\ninput and search for places nearby that fit the user’s desire. It was\nthe combination of a lot moving parts but with 10 hours straight of\nworking on it, we got it working.\n\n\nIn the end, we were able to not only win “Best Small Business\nSolution” but also “Best Overall”! It was extremely fun to work on this\nproject with friends, and if the opportunity presents itself in the\nfuture, I’d gladly refine and hopefully publish this project to the\nworld!\n\n\n\n",
      "last_modified": "2023-01-17T23:18:03+08:00"
    },
    {
      "path": "index.html",
      "title": "Personal Projects",
      "author": [],
      "contents": "\nAbout\n\n\n\n\nHello, I’m Nuo Wen Lei from Shanghai, current studying at Brown\nUniversity, class of 2026. My personal interests mainly lie in the realm\nof understanding humanities and the world around me through the lens of\ndata and logic.  I’m also a tennis and squash player (clip of me playing tennis) as\nwell as a Japanese-cuisine appreciator with ramen and chicken wings\nbeing my favorite.  This website is a gallery of the projects\nI’ve done. It shows how my interests developed over time from computer\nscience to web design to data science and AI.  I hope you find\nsomething interesting or just enjoy your time on this website!\n\nTable of Contents\nCOVID Death Trend Analysis and AI\nPrediction\nEpilepsy Detection AI Web\nApp\nHackathon Winning Project -\nExplorantine\nInvestment Bot\nMask Detection AI Web App\nPAC Donation Analysis Website\nR Tutorial Package Contribution\nSenior Project - Neural Network\nInterpreter\nShanghai Property Analysis\nWebsite\nWeChat Miniprogram - WangWang\nWordle and Quordle Solver\nQuick Overview\nGo\nto Projects tab for more details on each individual project\n\n\n\n\nCOVID Death Trend Analysis and\nAI Prediction  After an initial analysis of the COVID\ndeath trends and trial of basic AI methods, I dive deeper into finding\nother AI techniques to predict the death trends.\n\n\n\n\n\n\n\nEpilepsy Detection AI Web\nApp  I hosted an AI I created with friends that\ndetermines if someone is experiencing epilepsy based on EEG brain\nwaves.\n\n\n\n\n\n\n\nHackathon Winning Project -\nExplorantine  Our Hackathon team created a web app to\nhelp local businesses bring in customers.\n\n\n\n\n\n\n\nInvestment Bot\n I created a pipeline that analyzes web data and invests on an\nInvestopedia simulator.\n\n\n\n\n\n\n\nMask Detection AI Web\nApp  I created and hosted an AI on the internet that\ndetects if people are wearing masks in an image.\n\n\n\n\n\n\n\nPAC Donation Analysis\nWebsite  I analyzed and visualized the relationship\nbetween foreign-connected company donations with US Congress\nrepresentation.\n\n\n\n\n\n\n\nR Tutorial Package\nContribution  Over the summer of 2021, I work with a\nprofessor to help improve an R tutorial package.\n\n\n\n\n\n\n\nSenior Project - Model\nInterpreter  For my final spring semester in high\nschool, I aimed to create a neural network interpreter as my senior\nproject.\n\n\n\n\n\n\n\nShanghai Property Analysis\nWebsite  I created a website that visualizes the\nrelationship between properties and position in Shanghai.\n\n\n\n\n\n\n\nWeChat Miniprogram -\nWangWang  With friends, we are creating a WeChat\nminiprogram accessible to all WeChat users that can match tennis players\nwith others who play in the same area.\n\n\n\n\n\n\n\nWordle and Quordle AND OCTORDLE\nSolver  As a fun spring break project, I created an\nalgorithm that can live solve Wordle, Quordle, and Octordle with\nprobability.\n\n\n\n\n",
      "last_modified": "2023-01-17T23:18:04+08:00"
    },
    {
      "path": "investment_bot.html",
      "title": "Investment Bot",
      "author": [],
      "contents": "\nThis project started off as a personal challenge. During 2020, I took\na lot of online courses through sites like EdX and Coursera ranging from\nweb development to data science.\nI’ve made websites at this point, but I’ve never really applied my\nstudy in data science. This was essentially my first data science\nproject.\n\n\n\nFigure 1: Yahoo Finance page for the stock AXS\n\n\n\nUsing python and its various libraries, I created a simple pipeline\nto extract and analyze online data and then invest based on its results.\nThen I attached this pipeline to a web server that will run this\npipeline every week.\n\n\n\nFigure 2: Visualization of investment pipeline\n\n\n\nData Webscrape - The pipeline starts by taking\nvarious numerical and textual data from multiple online sources.\nData Cleaning and Processing - The data is then\ncleaned for processing use since these online sources meant for the data\nto be presented to people and not machines. In this section, features\nlike market cap values are also created based on their non-numeric\ndisplays on websites.\nInvestment Decision with ML - Using previously\ncollected data, many different ML models are trained, each with a\ndifferent model-scaler pairing. The pair with the highest evaluation\nscore is selected to transform the new data. Based on the confidence\nlevel of the predictions, the pipeline invests in the top 5 stock\nsymbols on an Investopedia simulator\n\n\n\n",
      "last_modified": "2023-01-17T23:18:04+08:00"
    },
    {
      "path": "mask_webapp.html",
      "title": "Mask Detection AI Web App",
      "author": [],
      "contents": "\nAssociated Links:\nMask Detection\nWeb App\nImage-processing AI Concept\nPresentation\nBy combining a face detection model and my personal mask detection\nmodel, I created an AI that can individually detect for each person in a\nphoto if they are wearing a mask.\n\n\n\nFigure 1: Preview of uploaded image before processing\n\n\n\nI then hosted this AI on a website for people to try out for\nthemselves. I also presented this project as a demo for a presentation about teaching AI\nconcepts to CA’s robotics club members.\nFeel free to try this AI out for yourself here.\nNote that it does take a while to load because the AI model size is\nbig.\n\n\n\nFigure 2: Result after mask detection AI processed image\n\n\n\n\n\n\n",
      "last_modified": "2023-01-17T23:18:05+08:00"
    },
    {
      "path": "pac_analysis.html",
      "title": "PAC Donation Analysis",
      "author": [],
      "contents": "\nAssociated Links:\nPAC Influence\nProject Website\nOpenSecrets\nFor the final project of a data science course about R, I picked my\nown topic of research to be about foreign-connected Political Action\nCommittee donations for US political parties.\nPlease feel free to visit the\nproject page and see for yourself.\n\n\n\nFigure 1: Data source for the project\n\n\n\nIn the project, I scrape data from OpenSecrets and process it to\nproduce visualizations comparing donation amounts and Congress\nrepresentations between 2000 and 2020.\n\n\n\n",
      "last_modified": "2023-01-17T23:18:05+08:00"
    },
    {
      "path": "r_package.html",
      "title": "R Data Science Tutorial Package Contribution",
      "author": [],
      "contents": "\nAssociated Links:\nPreceptor’s\nPrimer for Bayesian Data Science\nData Science Course\nhosted by professor David Kane\nClass\nManagement Documentation\nAssignment\nManagement Documentation\nAll\nmy code contributions\nAt the end of the 2021 school year, I joined a free data science course hosted\nby professor David Kane. During which he reached out to me to ask if\nI’d like to work with him over the summer to help improve his R tutorial\npackage, Preceptor’s\nPrimer for Bayesian Data Science.\n\n\n\nFigure 1: Chapter 1 of the Primer\n\n\n\nI gladly joined him with a few other volunteers. I started off by\njust catching and editing small mistakes in the tutorials reported by\nusers, then I got the opportunity to write the tutorial, Data\nWebscraping, myself. And afterwards, I was allowed to make some larger\nchanges to tutorials in general.\n\n\n\nFigure 2: Side navigation bar of the Data Webscraping\ntutorial\n\n\n\nFirst, I was assigned to add some quality-of-life features for better\nuser experience. Then I helped make the process of creating tutorials\nmore streamlined and simple by adding pre-defined templates and keyboard\nshortcuts to take the tedious work of writing each exercise individually\noff the tutorial-makers.\n\n\n\nFigure 3: My Github contributions to the package\n\n\n\nAfterwards, I worked on fixing a lot of the package’s technical\nerrors while adding more addins that check and format tutorials. Also, a\nrecent feature I am working on is creating a report of submitted answers\nfor users to look at after completing each tutorial.\n\n\n\nFigure 4: Sample Report\n\n\n\nAs the summer came to an end, I shifted towards working more on\ngeneral class and assignment management programs. With Discord templates\nand bots, I built a class\nsystem that allowed classes to be organized and conducted over\nDiscord in a similar fashion as over Zoom.\n\n\n\nFigure 5: Class Discord Server Template\n\n\n\nAnd the final assignment management features I added were what I\nthought to be my greatest contributions to the package. Using R packages\nthat allowed remote and automated access to Gmail and Google Drive, I\ncreated an assignment\ncollection, organization, and summarization system. With a single\nfunction, my code was able to collect the submissions from someone’s\nGmail, downloaded them, and aggregate user information such as name,\nemail, and time spent. After processing the submissions, the aggregated\ndata and submissions are uploaded to their Google Drive, allowing for\neasy access to the tallied information.\n\n\n\nFigure 6: Class Discord Server Template\n\n\n\nIt was truly an amazing experience to work so closely with professor\nDavid Kane over the summer. If there is ever another chance for me to do\nmeaningful work with him, I would gladly do so!\n\n\n\n",
      "last_modified": "2023-01-17T23:18:06+08:00"
    },
    {
      "path": "senior_project.html",
      "title": "Senior Project - Neural Network Interpreter",
      "author": [],
      "contents": "\nAssociated Links:\nGitHub\nRepo\nSchool\nPresentation\nOPTIMIZATION AS A\nMODEL FOR FEW-SHOT LEARNING\nATTENTION IS ALL YOU\nNEED\nThis is the senior project that I dedicated most of my spring\nsemester to. The focus is on creating a proof-of-concept AI model that\ncan interpret another AI model. Using word embeddings to understand\nfeatures and weights and biases between layers to understand\nrelationships between data points and neurons, I wanted to make an AI\nmodel that can “translate” the AI neuron weights to english, cracking\nopen the neural network blackbox.\n\n\n\nFigure 1: Interpreter Pipeline Structure\n\n\n\nThe way I landed on this particular idea was kind of through a\nmishmash of what I wanted to try and what problem I wanted to solve\n(mostly just what I wanted to try). I have always wanted to try making\nmodels that processed and produced the weirdest inputs and outputs. It\njust seems like when building models that processed images or sequences,\nI just slap on some CNNs or LSTMs and then call it a day, rarely\ncontributing anything of my own. Therefore, building this interpreter\nmodel is in a way like a challenge for myself. However, that is not to\nsay there is no application for a model like this.\n\n\n\nFigure 2: Difference in model interpretability between\nLogistic Regression (ML) and Neural Network (DL)\n\n\n\nThe image above compares the difference in model complexity and\ntherefore interpretability between a logistic regression model and a\nneural network model. You can see that for logistic regression or any\nkind of direct regression model for that matter, each input feature is\nassign a single weightage, representing how that feature contributes to\nthe output.\nHowever look to the right and you’ll see that it’s not as\nstraightforward for the neural network. If every line (edge) represents\na weightage, you’ll see that the neural network has quite a few more\nweightages than the logistic regression model. This is largely due to\nthe hidden layers between the inputs and output. These hidden layers\ncontain their own neurons whose meanings are defined and refined by the\nmodel. Therefore, encoded as numbers that only the model that created it\ncan understand, neural networks become like blackboxes, incomprehensible\nto humans.\nThis interpretability issue actually does impact company decisions to\nutilize and “trust” these machines because often times, especially in\nrecent years, the question of equity has been raised to many big data\nmodels. Companies need to make sure that their models do not treat\ndemographics unequitably, and so clarity within the model is very\nimportant. The neural network’s lack in that area thus often push big\ndata companies away from using deep learning solutions and resorting to\nmore basic machine learning solutions that produce decent results with\nclarity of feature importance.\nThis is the problem I wanted to solve with this model, some kind of\nmediator that can tell humans what the models are thinking.\n\n\n\nFigure 3: Word Embedding Visualization\n\n\n\nSo to make my model understand words, I encoded all the feature names\ninto word embeddings, which are vector points that represent word\nrelationships within a 300-dimensional vector space. To let my model\nunderstand word embeddings, I utilized the Multi Head Attention layer\nproposed in the paper, ATTENTION IS ALL YOU\nNEED and performed self attention with the layer. This allows the\nmodel to mix and understand how every dimension of the word embedding\nvectors relate to each other, thereby letting the model gain an even\nmore comprehensible understanding of the features.\n\n\n\nFigure 4: Multi Head Attention layer structure\n\n\n\nBy combining the understanding of words and linear weights, I created\n2 model types based on the mutability of input order. When thinking\nabout how to input the data, I realized that the model shouldn’t need to\nhave an immutable input order, as in an arbitrary feature “x” shouldn’t\nalways have to be in the same position of the batch. Therefore I first\ncreated a model structure that didn’t need a determined input order by\ntreating one data sample with all the different features and weights as\na batch with each sample being the corresponding feature and weight.\n\n\n\nFigure 5: Interpreter with dynamic input order\n\n\n\nThen, to see if an interpreter that assumes static input order yields\nany benefits, I created a static version too. This static version also\ntakes into account a covariance matrix between features and feature\nweights since it can assume static input order. The point of the matrix\nis that it should allow the interpreter to get a ballpark of where the\noutput feature should be in word embedding vector space.\n\n\n\nFigure 6: Interpreter with static input order\n\n\n\nIn terms of results, the static interpreter succeeded in training,\ncontinuing to improve even after some 250 steps while the dynamic\ninterpreter seems to have plateaued just after some 50 steps of\ntraining.\n\n\n\nFigure 7: Training results of the two interpreters\n\n\n\nHowever, after actually putting the interpreters into a testing\nenvironment, the results qualitatively changed drastically.\n\n\n\nFigure 8: Prediction results of the two interpreters\n\n\n\nBoth were given a base model that uses many attributes of a wine to\npredict the residual sugar in the wine. While the static interpreter\nrepeatedly predicted words that had little to no relation with “residual\nsugar”, the dynamic interpreter has predicted words that are similar to,\nif not exactly, “residual sugar”. I think the difference in training and\nprediction environment results is a testament to how overfitting can be\nsuch a misleading problem.\nAnyways, to conclude, this was a super fun project, guiding myself\nthrough everything. I feel like that proof-of-concept stage has been\nreached through this project. And while there are many ways to further\nthis project, I don’t think I am skilled enough to pursue many of those\noptions. So I’ll put the further exploration of this project on hold and\nsee if I can tackle it any better when I return to it.\nIf you want to see my presentation for this project, you can click\nhere.\nAnd here is the GitHub repo\nwith code for the project.\n\n\n\n",
      "last_modified": "2023-01-17T23:18:07+08:00"
    },
    {
      "path": "shanghai_property.html",
      "title": "Shanghai Property and Position Analysis",
      "author": [],
      "contents": "\nAssociated Links:\nShanghai\nProperty Analysis Project Page\nAs part of a school course about urban planning, I decided to analyze\nthe surrounding area of where I lived. It first started off as a small\narea analysis, but as my interest grew into Shanghai’s urban planning\nand as my direction of research became clearer, I scraped data from\nonline property-selling sites to produce a map that visualizes different\nattributes of properties and their positions in Shanghai.\n\n\n\nFigure 1: Distribution of district property colored by\nyear of construction (red: old, green: new)\n\n\n\nUsers can choose what attribute of the property to visualize and gain\nan insightful overview of the city distribution.\n\n\n\nFigure 2: Distribution of district property colored by\naverage price per square meter (red: low, green: high)\n\n\n\nWith this map, I also considered stakeholders and their\ninterconnectivity in this area.\n\n\n\nFigure 3: Graph of profit relationships between stakeholders\nin the Shanghai community\n\n\n\nOverall, I’d say this project in the scope of my school course was a\nsuccess. Through this experience, I’ve learned that the world of urban\nplanning is vast. This project has only glimpsed at the peak of the\niceberg and if opportunities arise in the future for me to dive deeper\ninto the hidden effects and influences of urban planning, I’d gladly\npursue it!\n\n\n\n",
      "last_modified": "2023-01-17T23:18:07+08:00"
    },
    {
      "path": "wangwang.html",
      "title": "WeChat Miniprogram - WangWang",
      "author": [],
      "contents": "\nA good friend and I started this project because we were both tennis\nplayers who had a hard time finding new players to play with near us.\nTherefore, we decided to make a WeChat miniprogram that does exactly\nthat. We decided to create it as a WeChat miniprogram to reach a large\naudience because WeChat is the main way of communication in China.\n\n\n\nFigure 1: Main Page of WangWang\n\n\n\nI handled the javascript backend programming to connect with the\ncloud database and my friend did all the styling and position for the\nfrontend. As we continued the project, another friend joined us and\ndesigned the layouts for all the pages. We are still currently working\non a development version, so unfortunately it is not usable yet, but we\nhave created several features already.\n\n\n\nFigure 2: Court Search Page of WangWang\n\n\n\nThe main feature is searching for courts near you, which gives you\ninformation about the court location and rating as well as how many\npeople play there.\n\n\n\nFigure 3: Profile Page of WangWang\n\n\n\nAnother feature we added is a personal profile page, which allows you\nto fill in your basic information for matching optimal tennis\npartners.\n\n\n\nFigure 4: Calendar Page of WangWang\n\n\n\nA feature we are still working on is a calendar function, which will\nallow users to better schedule meetings as well as further filter\nplayers based on parameters like available time.\nThough this project is still a work-in-progress, I have high hopes\nfor this miniprogram and I wish that in the near future I can also\npersonally use it!\n\n\n\n",
      "last_modified": "2023-01-17T23:18:08+08:00"
    },
    {
      "path": "wordle.html",
      "title": "Wordle and Quordle AND OCTORDLE Solver",
      "author": [],
      "contents": "\nAssociated Links:\nGitHub\nRepo\nWordle\nWordle\nArchive\nQuordle\nThis is a fun project I played around with over my senior year spring\nbreak. As the New York Times Wordle game took my school by storm, its\nsimple format made me curious about the possibility of creating an\nalgorithm to solve it. To start the project, I knew that I had to first\ncreate some interface for a machine to play Wordle, so, as pastime on a\nflight, I made a text version of Wordle. There is a small difference\nhowever. While the actual Wordle often displays words that are\nwell-known to people, I made my Wordle simply randomly choose a word out\nof all possible 5-letter words.\n\n\n\nFigure 1: Text Version of Wordle\n\n\n\nIn the text version, capitalize means the letter is in the right\nplace, “*” means the letter is present but in the wrong location, and\nlower case means the letter is not in the word. I also added a “Possible\nLetters” bar for ease of playing.\nAfter creating the Wordle game, I began to think about how I would\napproach creating a Wordle solver. Part of me wanted to create a\nReinforcement Learning agent to play the game, but another part of me\nfelt that I’ve been doing too much AI-related projects recently. I\nsettled on creating a non-AI algorithm that would probabilistically\nchoose the optimal words.\n\n\n\nFigure 2: Exploration of letter-location probability at the\nfirst position of a word\n\n\n\nI first made an algorithm that assigned information “rewards” to\ngetting a letter correct (3 points), present (2 points), or wrong (1\npoint), thereby generating an information reward value for each word in\nthe vocabulary. It then simply chose the word with the highest\ninformation reward. This algorithm created a machine that prioritized\nletters with a high probability of being in that position above all\nelse. And while it did work to an extent, it was more of a Wordle\n“attempter” than a Wordle “solver”.\nNow if I really wanted to spin this into a story about personal\ninsights and biases, this would be the moral of the story. I assumed\nthat a letter in the correct place would always provide more information\nthan other results, and in doing so I created an algorithm whose goal\ndeviated from the overarching goal.\nTherefore, to improve the machine, firstly I changed the information\nreward as dependent on what percentage of the vocabulary the action\nwould eliminate. For example, if getting the word correct eliminates\nhalf the vocab, its reward for being correct was 1/2. This is somewhat\nof a greedy solution because by evaluating the effect of each letter\nindividually, I ignore the percentage of vocabulary that would be\neliminated by the outcomes of 2 or more letters in the word. This\noverlap-elimination effect, however, is minimal, so I chose to ignore\nthe flaw.\nSecondly, I added a bunch of conditionals to make sure that no\ninformation was wasted. I found that covering all the scenarios where a\nletter that’s present but in the wrong position was quite challenging.\nBut one-by-one I found and covered all those edge cases. Finally, I set\nthe machine to use the first 5 guesses to eliminate as much of the\nvocabulary as possible and then use the most possible word only for the\nfinal guess. This allowed the algorithm to truly “solve” Wordle.\nI then created a web interface for the machine to actually play the\nofficial Wordle. Using the Python module of Selenium, the Wordle solver\n(named Trish) could solve the Wordle live in front of your eyes!\n\n\n\nFigure 3: Trish solving Wordle for March 20, 2022\n\n\n\nIt can also solve any Wordle archived here.\n\n\n\nFigure 4: Trish solving Wordle for March 20, 2022\n\n\n\nAs a final challenge, I created a separate algorithm that solved Quordle instead of Wordle (named\nTrisha) by simply adding up the information reward value that could be\ngained from every board and left the last 4 guesses for optimal words.\nAnd, well, as of now (March 20, 2022), it hasn’t failed yet!\n\n\n\nFigure 5: Trisha solving Quordle for March 20, 2022\n\n\n\nDid I say final challenge? I lied. A good friend told me that an\nOctordle exists, so I had to try it out. The previous algorithm actually\nFAILED on its first attempt to solve the daily, and I realized it was\nbecause the model didn’t prioritize game boards with more vocabulary to\nfilter. Therefore, I added a scaling component that multiplies with the\ninformation reward based on the number of vocabulary for each game\nboard. This changes the reward gotten from each game board, allowing the\nsolver (now named Patricia) to have a sense of priority or urgency.\n\n\n\nFigure 6: Patricia solving Octordle for March 20, 2022\n\n\n\nThis just started off as a fun project for over spring break.\nHowever, it not only helped me brush up on actual hand-coding, but it\nalso taught me some good lessons about probabilities and my own\nassumptions!\n\n\n\n",
      "last_modified": "2023-01-17T23:18:09+08:00"
    }
  ],
  "collections": []
}
